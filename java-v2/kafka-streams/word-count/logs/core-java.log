2024-08-12 23:22:55 INFO  StreamsConfig:372 - StreamsConfig values: 
	acceptable.recovery.lag = 10000
	application.id = word-count
	application.server = 
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:29092]
	buffered.records.per.partition = 1000
	built.in.metrics.version = latest
	cache.max.bytes.buffering = 10485760
	client.id = 
	commit.interval.ms = 100
	connections.max.idle.ms = 540000
	default.client.supplier = class org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier
	default.deserialization.exception.handler = class org.apache.kafka.streams.errors.LogAndFailExceptionHandler
	default.dsl.store = rocksDB
	default.key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	default.list.key.serde.inner = null
	default.list.key.serde.type = null
	default.list.value.serde.inner = null
	default.list.value.serde.type = null
	default.production.exception.handler = class org.apache.kafka.streams.errors.DefaultProductionExceptionHandler
	default.timestamp.extractor = class org.apache.kafka.streams.processor.FailOnInvalidTimestamp
	default.value.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	dsl.store.suppliers.class = class org.apache.kafka.streams.state.BuiltInDslStoreSuppliers$RocksDBDslStoreSuppliers
	enable.metrics.push = true
	max.task.idle.ms = 0
	max.warmup.replicas = 2
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	poll.ms = 100
	probing.rebalance.interval.ms = 600000
	processing.guarantee = exactly_once_v2
	rack.aware.assignment.non_overlap_cost = null
	rack.aware.assignment.strategy = none
	rack.aware.assignment.tags = []
	rack.aware.assignment.traffic_cost = null
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	repartition.purge.interval.ms = 30000
	replication.factor = -1
	request.timeout.ms = 40000
	retries = 0
	retry.backoff.ms = 100
	rocksdb.config.setter = null
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	state.cleanup.delay.ms = 600000
	state.dir = C:\one-place\practice\kafka-streams
	statestore.cache.max.bytes = 0
	task.assignor.class = null
	task.timeout.ms = 300000
	topology.optimization = none
	upgrade.from = null
	window.size.ms = null
	windowed.inner.class.serde = null
	windowstore.changelog.additional.retention.ms = 86400000

2024-08-12 23:22:55 ERROR StateDirectory:163 - Failed to change permissions for the directory C:\one-place\practice\kafka-streams
2024-08-12 23:22:55 ERROR StateDirectory:163 - Failed to change permissions for the directory C:\one-place\practice\kafka-streams\word-count
2024-08-12 23:22:55 INFO  StateDirectory:205 - Reading UUID from process file: 05835364-eb20-4c1e-be25-2fe1994d5da7
2024-08-12 23:22:56 INFO  AdminClientConfig:372 - AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:29092]
	client.dns.lookup = use_all_dns_ips
	client.id = word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2024-08-12 23:22:56 INFO  AppInfoParser:124 - Kafka version: 3.8.0
2024-08-12 23:22:56 INFO  AppInfoParser:125 - Kafka commitId: 771b9576b00ecf5b
2024-08-12 23:22:56 INFO  AppInfoParser:126 - Kafka startTimeMs: 1723485176557
2024-08-12 23:22:56 INFO  KafkaStreams:998 - stream-client [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7] Kafka Streams version: 3.8.0
2024-08-12 23:22:56 INFO  KafkaStreams:999 - stream-client [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7] Kafka Streams commit ID: 771b9576b00ecf5b
2024-08-12 23:22:56 INFO  StreamThread:383 - stream-thread [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1] Creating restore consumer client
2024-08-12 23:22:56 INFO  ConsumerConfig:372 - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-restore-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = true
	isolation.level = read_committed
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2024-08-12 23:22:56 INFO  KafkaMetricsCollector:269 - initializing Kafka metrics collector
2024-08-12 23:22:56 INFO  AppInfoParser:124 - Kafka version: 3.8.0
2024-08-12 23:22:56 INFO  AppInfoParser:125 - Kafka commitId: 771b9576b00ecf5b
2024-08-12 23:22:56 INFO  AppInfoParser:126 - Kafka startTimeMs: 1723485176809
2024-08-12 23:22:56 INFO  StreamThread:104 - stream-thread [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1] Creating thread producer client
2024-08-12 23:22:56 INFO  ProducerConfig:372 - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:29092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-producer
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = true
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 10000
	transactional.id = word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-1
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2024-08-12 23:22:56 INFO  KafkaMetricsCollector:269 - initializing Kafka metrics collector
2024-08-12 23:22:56 INFO  KafkaProducer:619 - [Producer clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-producer, transactionalId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-1] Instantiated a transactional producer.
2024-08-12 23:22:56 INFO  AppInfoParser:124 - Kafka version: 3.8.0
2024-08-12 23:22:56 INFO  AppInfoParser:125 - Kafka commitId: 771b9576b00ecf5b
2024-08-12 23:22:56 INFO  AppInfoParser:126 - Kafka startTimeMs: 1723485176944
2024-08-12 23:22:56 INFO  DefaultStateUpdater:144 - state-updater [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StateUpdater-1] State updater thread started
2024-08-12 23:22:56 INFO  StreamThread:461 - stream-thread [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1] Creating consumer client
2024-08-12 23:22:56 INFO  ConsumerConfig:372 - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:29092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = word-count
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = false
	internal.throw.on.fetch.stable.offset.unsupported = true
	isolation.level = read_committed
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 1000
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

2024-08-12 23:22:56 INFO  KafkaMetricsCollector:269 - initializing Kafka metrics collector
2024-08-12 23:22:57 INFO  AssignorConfiguration:144 - stream-thread [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer] Cooperative rebalancing protocol is enabled now
2024-08-12 23:22:57 INFO  ConsumerConfig:381 - These configurations '[replication.factor, probing.rebalance.interval.ms, max.warmup.replicas, acceptable.recovery.lag, task.assignor.class, rack.aware.assignment.non_overlap_cost, application.server, rack.aware.assignment.strategy, rack.aware.assignment.traffic_cost, windowstore.changelog.additional.retention.ms, num.standby.replicas, upgrade.from, rack.aware.assignment.tags, application.id]' were supplied but are not used yet.
2024-08-12 23:22:57 INFO  AppInfoParser:124 - Kafka version: 3.8.0
2024-08-12 23:22:57 INFO  AppInfoParser:125 - Kafka commitId: 771b9576b00ecf5b
2024-08-12 23:22:57 INFO  AppInfoParser:126 - Kafka startTimeMs: 1723485177084
2024-08-12 23:22:57 INFO  KafkaStreams:346 - stream-client [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7] State transition from CREATED to REBALANCING
2024-08-12 23:22:57 INFO  KafkaStreams:1411 - stream-client [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7] Started 1 stream threads
2024-08-12 23:22:57 INFO  StreamThread:663 - stream-thread [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1] Starting
2024-08-12 23:22:57 INFO  StreamThread:250 - stream-thread [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1] State transition from CREATED to STARTING
2024-08-12 23:22:57 INFO  LegacyKafkaConsumer:476 - [Consumer clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer, groupId=word-count] Subscribed to topic(s): word-count-KSTREAM-AGGREGATE-STATE-STORE-0000000004-repartition, word-count-input
2024-08-12 23:22:57 INFO  StreamThread:1074 - stream-thread [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1] Processed 0 total records, ran 0 punctuators, and committed 0 total tasks since the last update
2024-08-12 23:22:57 INFO  Metadata:364 - [Producer clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-producer, transactionalId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-1] Cluster ID: _7scHowBSayaimOYAv4cJg
2024-08-12 23:22:57 INFO  Metadata:364 - [Consumer clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer, groupId=word-count] Cluster ID: _7scHowBSayaimOYAv4cJg
2024-08-12 23:22:57 INFO  ConsumerCoordinator:936 - [Consumer clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer, groupId=word-count] Discovered group coordinator localhost:29092 (id: 2147483646 rack: null)
2024-08-12 23:22:57 INFO  ConsumerCoordinator:604 - [Consumer clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer, groupId=word-count] (Re-)joining group
2024-08-12 23:22:57 INFO  ConsumerCoordinator:1102 - [Consumer clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer, groupId=word-count] Request joining group due to: need to re-join with the given member-id: word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer-c2ff1807-55c2-4a66-bac5-92c42ebfcf16
2024-08-12 23:22:57 INFO  ConsumerCoordinator:604 - [Consumer clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer, groupId=word-count] (Re-)joining group
2024-08-12 23:23:00 INFO  ConsumerCoordinator:665 - [Consumer clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer, groupId=word-count] Successfully joined group with generation Generation{generationId=3, memberId='word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer-c2ff1807-55c2-4a66-bac5-92c42ebfcf16', protocol='stream'}
2024-08-12 23:23:00 WARN  KafkaAdminClient:2293 - [AdminClient clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-admin] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics.
2024-08-12 23:23:00 WARN  KafkaAdminClient:2293 - [AdminClient clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-admin] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics.
2024-08-12 23:23:00 INFO  StreamsPartitionAssignor:778 - stream-thread [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer] 1 client nodes and 1 consumers participating in this rebalance: 
05835364-eb20-4c1e-be25-2fe1994d5da7: [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer-c2ff1807-55c2-4a66-bac5-92c42ebfcf16].
2024-08-12 23:23:00 INFO  StreamsPartitionAssignor:789 - stream-thread [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer] Assigning stateful tasks: [1_0]
and stateless tasks: [0_0]
2024-08-12 23:23:00 INFO  AssignorConfiguration:259 - stream-thread [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer] No custom task assignors found, defaulting to internal task assignment with internal.task.assignor.class
2024-08-12 23:23:00 INFO  HighAvailabilityTaskAssignor:107 - Decided on assignment: {05835364-eb20-4c1e-be25-2fe1994d5da7=[activeTasks: ([0_0, 1_0]) standbyTasks: ([]) prevActiveTasks: ([]) prevStandbyTasks: ([]) changelogOffsetTotalsByTask: ([]) taskLagTotals: ([1_0=0]) clientTags: ([]) capacity: 1 assigned: 2]} with no followup probing rebalance.
2024-08-12 23:23:00 INFO  StreamsPartitionAssignor:850 - stream-thread [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer] Assigned 2 total tasks including 1 stateful tasks to 1 client nodes.
2024-08-12 23:23:00 INFO  StreamsPartitionAssignor:854 - stream-thread [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer] Assignment of tasks to nodes: 05835364-eb20-4c1e-be25-2fe1994d5da7=[activeTasks: ([0_0, 1_0]) standbyTasks: ([])]
2024-08-12 23:23:00 INFO  StreamsPartitionAssignor:1081 - stream-thread [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer] Client 05835364-eb20-4c1e-be25-2fe1994d5da7 per-consumer assignment:
	prev owned active {}
	prev owned standby {word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer-c2ff1807-55c2-4a66-bac5-92c42ebfcf16=[]}
	assigned active {word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer-c2ff1807-55c2-4a66-bac5-92c42ebfcf16=[1_0, 0_0]}
	revoking active {}
	assigned standby {}

2024-08-12 23:23:00 INFO  StreamsPartitionAssignor:1100 - stream-thread [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer] Finished stable assignment of tasks, no followup rebalances required.
2024-08-12 23:23:00 INFO  ConsumerCoordinator:663 - [Consumer clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer, groupId=word-count] Finished assignment for group at generation 3: {word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer-c2ff1807-55c2-4a66-bac5-92c42ebfcf16=Assignment(partitions=[word-count-KSTREAM-AGGREGATE-STATE-STORE-0000000004-repartition-0, word-count-input-0], userDataSize=64)}
2024-08-12 23:23:00 INFO  ConsumerCoordinator:842 - [Consumer clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer, groupId=word-count] Successfully synced group in generation Generation{generationId=3, memberId='word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer-c2ff1807-55c2-4a66-bac5-92c42ebfcf16', protocol='stream'}
2024-08-12 23:23:00 INFO  ConsumerCoordinator:386 - [Consumer clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer, groupId=word-count] Updating assignment with
	Assigned partitions:                       [word-count-KSTREAM-AGGREGATE-STATE-STORE-0000000004-repartition-0, word-count-input-0]
	Current owned partitions:                  []
	Added partitions (assigned - owned):       [word-count-KSTREAM-AGGREGATE-STATE-STORE-0000000004-repartition-0, word-count-input-0]
	Revoked partitions (owned - assigned):     []

2024-08-12 23:23:00 INFO  ConsumerCoordinator:323 - [Consumer clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer, groupId=word-count] Notifying assignor about the new Assignment(partitions=[word-count-KSTREAM-AGGREGATE-STATE-STORE-0000000004-repartition-0, word-count-input-0], userDataSize=64)
2024-08-12 23:23:00 INFO  StreamsPartitionAssignor:1581 - stream-thread [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer] No followup rebalance was requested, resetting the rebalance schedule.
2024-08-12 23:23:00 INFO  TaskManager:335 - stream-thread [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1] Handle new assignment with:
	New active tasks: [1_0, 0_0]
	New standby tasks: []
	Existing active tasks: []
	Existing standby tasks: []
2024-08-12 23:23:00 INFO  ConsumerRebalanceListenerInvoker:57 - [Consumer clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer, groupId=word-count] Adding newly assigned partitions: word-count-KSTREAM-AGGREGATE-STATE-STORE-0000000004-repartition-0, word-count-input-0
2024-08-12 23:23:00 INFO  StreamThread:250 - stream-thread [word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1] State transition from STARTING to PARTITIONS_ASSIGNED
2024-08-12 23:23:00 INFO  ConsumerCoordinator:1506 - [Consumer clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer, groupId=word-count] Found no committed offset for partition word-count-input-0
2024-08-12 23:23:00 INFO  ConsumerCoordinator:1506 - [Consumer clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer, groupId=word-count] Found no committed offset for partition word-count-KSTREAM-AGGREGATE-STATE-STORE-0000000004-repartition-0
2024-08-12 23:23:00 INFO  SubscriptionState:398 - [Consumer clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer, groupId=word-count] Resetting offset for partition word-count-input-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:29092 (id: 1 rack: null)], epoch=0}}.
2024-08-12 23:23:00 INFO  SubscriptionState:398 - [Consumer clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-consumer, groupId=word-count] Resetting offset for partition word-count-KSTREAM-AGGREGATE-STATE-STORE-0000000004-repartition-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:29092 (id: 1 rack: null)], epoch=0}}.
2024-08-12 23:23:00 INFO  TransactionManager:291 - [Producer clientId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-StreamThread-1-producer, transactionalId=word-count-05835364-eb20-4c1e-be25-2fe1994d5da7-1] Invoking InitProducerId for the first time in order to acquire a producer ID
2024-08-12 23:24:00 WARN  StreamsProducer:170 - stream-thread [main] Timeout exception caught trying to initialize transactions. The broker is either slow or in bad state (like not having enough replicas) in responding to the request, or the connection to broker was interrupted sending the request or receiving the response. Will retry initializing the task in the next loop. Consider overwriting max.block.ms to a larger value to avoid timeout errors
